from flask import Flask, request, jsonify, Response
import requests
import json
from datetime import datetime, timedelta
import os
import time
import random
from flask_cors import CORS
import threading
import logging
import schedule  # Import pentru programarea task-urilor
import concurrent.futures  # Pentru rularea task-urilor Ã®n paralel

# Configurare logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

app = Flask(__name__)
CORS(app)  # Activare CORS pentru toate rutele

# Cache pentru stocarea rÄƒspunsurilor API
cache = {}
cache_lock = threading.Lock()  # Lock pentru operaÈ›iuni thread-safe pe cache
cache_duration = 1800  # MÄƒresc durata cache-ului la 30 minute pentru a reduce radical cererile
cache_refresh_interval = 900  # ReÃ®mprospÄƒtare Ã®n fundal la fiecare 15 minute

# Rate limiter pentru a limita numÄƒrul de cereri care ajung la API
# LimitÄƒm la maxim 1 cerere per API la fiecare 5 secunde
rate_limits = {}
rate_limit_lock = threading.Lock()
RATE_LIMIT_INTERVAL = 5  # secunde Ã®ntre cereri cÄƒtre acelaÈ™i API

# Configurare pentru retry È™i rate limiting
MAX_RETRIES = 5  # MÄƒresc numÄƒrul de Ã®ncercÄƒri
RETRY_DELAY = 2  # secunde
USE_LAST_VALID_RESPONSE = True  # ReturnÄƒm ultimul rÄƒspuns valid din cache cÃ¢nd API-ul eÈ™ueazÄƒ

# Configurare pentru prefetch
PREFETCH_ENABLED = True
IMPORTANT_ENDPOINTS = [
    {
        "name": "bitcoin_price",
        "url": "https://api.coingecko.com/api/v3/coins/markets",
        "params": {
            "vs_currency": "usd",
            "ids": "bitcoin,ethereum",
            "order": "market_cap_desc",
            "per_page": "50",
            "page": "1",
            "sparkline": "false",
            "price_change_percentage": "24h"
        },
        "processor": None  # Va folosi procesarea implicitÄƒ
    },
    {
        "name": "global_market",
        "url": "https://api.coingecko.com/api/v3/global",
        "params": None,
        "processor": None
    },
    {
        "name": "trending_coins",
        "url": "https://api.coingecko.com/api/v3/search/trending",
        "params": None,
        "processor": None
    }
]

# Date de backup pentru cÃ¢nd toate Ã®ncercÄƒrile eÈ™ueazÄƒ
FALLBACK_PRICES_DATA = [
    {
        "id": "bitcoin",
        "symbol": "btc",
        "name": "Bitcoin",
        "image": "https://assets.coingecko.com/coins/images/1/small/bitcoin.png",
        "current_price": 95940,
        "market_cap": 750000000000,
        "market_cap_rank": 1,
        "total_volume": 21900000000,
        "price_change_percentage_24h": -1.03,
        "sparkline_in_7d": {"price": []}
    },
    {
        "id": "ethereum",
        "symbol": "eth",
        "name": "Ethereum",
        "image": "https://assets.coingecko.com/coins/images/279/small/ethereum.png",
        "current_price": 1825.0,
        "market_cap": 265000000000,
        "market_cap_rank": 2,
        "total_volume": 10500000000,
        "price_change_percentage_24h": -0.47,
        "sparkline_in_7d": {"price": []}
    }
]

@app.route('/')
def home():
    """Ruta principalÄƒ cu instrucÈ›iuni de bazÄƒ"""
    return """
    <html>
        <head>
            <title>Server Proxy API</title>
            <style>
                body { font-family: Arial, sans-serif; max-width: 800px; margin: 40px auto; padding: 0 20px; line-height: 1.6; }
                h1 { color: #333; }
                .endpoint { margin-bottom: 20px; padding: 15px; background-color: #f9f9f9; border-left: 4px solid #3498db; }
                code { background-color: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; }
                .status { margin-top: 30px; padding: 10px; background-color: #e8f7f0; border-radius: 5px; }
                .status-good { color: #2ecc71; }
                .prefetch { margin-top: 20px; background-color: #f0f8ff; padding: 10px; border-radius: 5px; }
                .stats { display: flex; justify-content: space-between; margin-top: 20px; }
                .stat-box { flex: 1; margin: 0 10px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; text-align: center; }
                .stat-box h4 { margin-top: 0; }
            </style>
        </head>
        <body>
            <h1>Server Proxy API pentru Crypto Platform</h1>
            <p>Acest server proxy ajutÄƒ la reducerea numÄƒrului de cereri cÄƒtre API-uri externe.</p>
            
            <div class="endpoint">
                <h3>1. API Proxy General: <code>/api/proxy?url=URL_ENCODED</code></h3>
                <p>Exemplu: <code>/api/proxy?url=https://api.coingecko.com/api/v3/coins/bitcoin</code></p>
            </div>
            
            <div class="endpoint">
                <h3>2. Cereri Ã®n Lot: <code>/api/batch</code> (POST)</h3>
                <p>Trimite un array de URL-uri pentru procesare simultanÄƒ</p>
            </div>
            
            <div class="endpoint">
                <h3>3. PreÈ›uri Criptomonede: <code>/api/prices</code></h3>
                <p>Endpoint optimizat pentru preÈ›urile criptomonedelor</p>
                <p>Parametri opÈ›ionali: vs_currency, order, per_page, page, sparkline, ids</p>
            </div>
            
            <div class="endpoint">
                <h3>4. Golire Cache: <code>/api/cache/clear</code> (POST)</h3>
                <p>ForÈ›eazÄƒ reÃ®mprospÄƒtarea datelor din cache</p>
            </div>
            
            <div class="endpoint">
                <h3>5. Stare Cache: <code>/api/cache/status</code></h3>
                <p>VerificÄƒ starea È™i dimensiunea cache-ului</p>
            </div>
            
            <div class="prefetch">
                <h3>Prefetch Automat</h3>
                <p>Serverul extrage automat datele urmÄƒtoarelor endpoint-uri la fiecare 15 minute:</p>
                <ul>
                    <li>PreÈ›uri Bitcoin & Ethereum</li>
                    <li>Stare PiaÈ›Äƒ GlobalÄƒ</li>
                    <li>Criptomonede Populare</li>
                </ul>
                <p>Acest mecanism asigurÄƒ cÄƒ datele sunt mereu disponibile chiar dacÄƒ API-urile externe au probleme.</p>
            </div>
            
            <div class="status">
                <h3>Stare server: <span class="status-good">Activ âœ“</span></h3>
                <div class="stats">
                    <div class="stat-box">
                        <h4>Cache</h4>
                        <div id="cache-count">0</div>
                        <div style="font-size: 12px; opacity: 0.7;">intrÄƒri</div>
                    </div>
                    <div class="stat-box">
                        <h4>Durata Cache</h4>
                        <div>30</div>
                        <div style="font-size: 12px; opacity: 0.7;">minute</div>
                    </div>
                    <div class="stat-box">
                        <h4>Ultima Actualizare</h4>
                        <div id="last-update" style="font-size: 14px;">""" + datetime.now().strftime("%H:%M:%S") + """</div>
                        <div style="font-size: 12px; opacity: 0.7;">""" + datetime.now().strftime("%Y-%m-%d") + """</div>
                    </div>
                </div>
                <script>
                    // Auto-refresh pentru pagina home
                    setInterval(() => {
                        fetch('/api/cache/status')
                            .then(res => res.json())
                            .then(data => {
                                document.getElementById('cache-count').textContent = data.cache_entries;
                                document.getElementById('last-update').textContent = data.last_update.split(' ')[1];
                            });
                    }, 5000);
                </script>
            </div>
        </body>
    </html>
    """

def get_random_headers():
    """GenereazÄƒ headere aleatorii pentru a evita detectarea ca bot"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
    ]
    
    return {
        'User-Agent': random.choice(user_agents),
        'Accept': 'application/json',
        'Accept-Language': 'en-US,en;q=0.9',
        'Referer': 'https://www.google.com/',
        'Origin': 'https://www.google.com',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }

def can_make_request(api_domain):
    """VerificÄƒ dacÄƒ se poate face o cerere cÄƒtre un anumit API folosind rate limiting"""
    with rate_limit_lock:
        current_time = time.time()
        
        # Extragem domeniul din URL pentru rate limiting
        if api_domain not in rate_limits:
            rate_limits[api_domain] = 0
            return True
        
        # VerificÄƒm dacÄƒ a trecut suficient timp de la ultima cerere
        if current_time - rate_limits[api_domain] >= RATE_LIMIT_INTERVAL:
            rate_limits[api_domain] = current_time
            return True
        
        return False

def make_request_with_retry(url, params=None):
    """FuncÈ›ie pentru a face cereri cu retry È™i gestionare avansatÄƒ a erorilor"""
    # Extragem domeniul pentru rate limiting
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    api_domain = parsed_url.netloc
    
    # VerificÄƒm rate limit-ul
    if not can_make_request(api_domain):
        logging.warning(f"âš ï¸ Rate limit local atins pentru {api_domain}, aÈ™teptÄƒm...")
        time.sleep(RATE_LIMIT_INTERVAL)
    
    for attempt in range(MAX_RETRIES):
        try:
            # AdÄƒugÄƒm header-uri pentru a evita detecÈ›ia ca bot
            headers = get_random_headers()
            
            # AdÄƒugÄƒm un delay aleator pentru a evita detecÈ›ia de cereri automatizate
            if attempt > 0:
                time.sleep(RETRY_DELAY * (1 + attempt))
            
            # Facem cererea cu timeout È™i verificare SSL
            response = requests.get(
                url,
                params=params,
                headers=headers,
                timeout=10,
                verify=True
            )
            
            # VerificÄƒm codul de rÄƒspuns
            if response.status_code == 429:  # Too Many Requests
                logging.warning(f"âš ï¸ Rate limit API atins pentru {api_domain}, aÈ™teptÄƒm...")
                time.sleep(RETRY_DELAY * (2 ** attempt))  # Exponential backoff
                continue
                
            response.raise_for_status()
            
            # ÃncercÄƒm sÄƒ decodÄƒm JSON-ul
            try:
                data = response.json()
                logging.info(f"âœ… Cerere reuÈ™itÄƒ cÄƒtre {api_domain}")
                return data
            except json.JSONDecodeError as e:
                logging.error(f"âŒ Eroare decodare JSON de la {api_domain}: {e}")
                if attempt == MAX_RETRIES - 1:
                    raise
                continue
                
        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ Eroare cerere cÄƒtre {api_domain}: {e}")
            if attempt == MAX_RETRIES - 1:
                raise
            continue
            
    raise Exception(f"Toate Ã®ncercÄƒrile au eÈ™uat pentru {url}")

def get_cache_with_lock(key, default=None):
    """ObÈ›ine o valoare din cache cu lock pentru thread safety"""
    with cache_lock:
        return cache.get(key, default)

def set_cache_with_lock(key, value):
    """SeteazÄƒ o valoare Ã®n cache cu lock pentru thread safety"""
    with cache_lock:
        cache[key] = value

def prefetch_important_data():
    """
    FuncÈ›ie pentru a prelua È™i stoca Ã®n cache datele importante Ã®n fundal,
    astfel Ã®ncÃ¢t sÄƒ fie Ã®ntotdeauna disponibile.
    """
    if not PREFETCH_ENABLED:
        return
    
    logging.info("ğŸ”„ Ãncepe prefetch pentru endpoint-urile importante...")
    
    # Folosim un executor pentru a face cererile Ã®n paralel
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        
        for endpoint in IMPORTANT_ENDPOINTS:
            futures.append(
                executor.submit(
                    prefetch_single_endpoint,
                    endpoint["name"],
                    endpoint["url"],
                    endpoint["params"],
                    endpoint["processor"]
                )
            )
        
        # AÈ™teptÄƒm terminarea tuturor cererilor
        for future in concurrent.futures.as_completed(futures):
            try:
                endpoint_name, success = future.result()
                if success:
                    logging.info(f"âœ… Prefetch reuÈ™it pentru {endpoint_name}")
                else:
                    logging.warning(f"âš ï¸ Prefetch eÈ™uat pentru {endpoint_name}")
            except Exception as e:
                logging.error(f"âŒ Eroare la prefetch: {e}")
    
    logging.info(f"ğŸ”„ Prefetch complet. UrmÄƒtorul prefetch Ã®n {cache_refresh_interval // 60} minute.")

def prefetch_single_endpoint(name, url, params, processor):
    """
    Face un singur prefetch pentru un endpoint specific.
    ReturneazÄƒ numele endpoint-ului È™i un boolean care indicÄƒ succesul.
    """
    try:
        # ConstruieÈ™te cache key-ul
        if params:
            query_string = "&".join([f"{k}={v}" for k, v in params.items()])
            cache_key = f"{url}?{query_string}"
        else:
            cache_key = url
        
        # Face cererea
        data = make_request_with_retry(url, params)
        
        # AplicÄƒ procesarea dacÄƒ este specificatÄƒ
        if processor and callable(processor):
            data = processor(data)
        
        # SalveazÄƒ Ã®n cache
        set_cache_with_lock(cache_key, (datetime.now(), data))
        
        return name, True
    except Exception as e:
        logging.error(f"âŒ Eroare la prefetch pentru {name}: {e}")
        return name, False

def schedule_prefetch_jobs():
    """ProgrameazÄƒ job-urile de prefetch"""
    if PREFETCH_ENABLED:
        # RulÄƒm imediat la pornirea serverului
        threading.Thread(target=prefetch_important_data).start()
        
        # ProgramÄƒm sÄƒ ruleze regulat
        schedule.every(cache_refresh_interval).seconds.do(
            lambda: threading.Thread(target=prefetch_important_data).start()
        )
        
        # Thread separat pentru a rula scheduler-ul
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(1)
        
        scheduler_thread = threading.Thread(target=run_scheduler)
        scheduler_thread.daemon = True
        scheduler_thread.start()
        
        logging.info(f"ğŸ•’ Prefetch automat programat la fiecare {cache_refresh_interval // 60} minute")

@app.route('/api/proxy', methods=['GET'])
def proxy():
    """
    Endpoint proxy pentru apeluri API.
    Parametri:
    - url: URL-ul API-ului È›intÄƒ
    - cache_time: timpul opÈ›ional de caching Ã®n secunde
    """
    target_url = request.args.get('url')
    if not target_url:
        return jsonify({'error': 'URL parametru lipsÄƒ'}), 400
    
    # VerificÄƒ timpul de cache personalizat
    cache_time = request.args.get('cache_time')
    if cache_time and cache_time.isdigit():
        current_cache_duration = int(cache_time)
    else:
        current_cache_duration = cache_duration
    
    # VerificÄƒ dacÄƒ rÄƒspunsul este Ã®n cache È™i dacÄƒ este Ã®ncÄƒ valid
    cached_data = get_cache_with_lock(target_url)
    if cached_data:
        cache_time, cache_data = cached_data
        if datetime.now() < cache_time + timedelta(seconds=current_cache_duration):
            logging.info(f"âœ… Folosesc date din cache pentru {target_url[:50]}...")
            return jsonify(cache_data)
    
    # DacÄƒ nu existÄƒ Ã®n cache sau a expirat, face cererea API
    try:
        logging.info(f"ğŸŒ Preiau date noi pentru {target_url[:50]}...")
        data = make_request_with_retry(target_url)
        
        # SalveazÄƒ rÄƒspunsul Ã®n cache
        set_cache_with_lock(target_url, (datetime.now(), data))
        
        return jsonify(data)
    except Exception as e:
        logging.error(f"âŒ Eroare la preluarea datelor: {e}")
        
        # DacÄƒ avem date Ã®n cache, le returnÄƒm pe acelea chiar dacÄƒ au expirat
        cached_data = get_cache_with_lock(target_url)
        if USE_LAST_VALID_RESPONSE and cached_data:
            logging.warning(f"âš ï¸ Returnez ultimele date valide din cache pentru {target_url[:50]}")
            return jsonify(cached_data[1])
        
        return jsonify({'error': str(e), 'message': 'Nu s-au putut prelua date noi È™i nu existÄƒ date Ã®n cache'}), 500

@app.route('/api/batch', methods=['POST'])
def batch():
    """
    Endpoint pentru cereri API Ã®n lot.
    AÈ™teaptÄƒ un array de URL-uri Ã®n corpul cererii.
    """
    data = request.get_json()
    if not data or 'urls' not in data:
        return jsonify({'error': 'Array-ul de URL-uri lipsÄƒ'}), 400
    
    urls = data['urls']
    results = {}
    
    for url in urls:
        # VerificÄƒ dacÄƒ rÄƒspunsul este Ã®n cache È™i dacÄƒ este Ã®ncÄƒ valid
        cached_data = get_cache_with_lock(url)
        if cached_data:
            cache_time, cache_data = cached_data
            if datetime.now() < cache_time + timedelta(seconds=cache_duration):
                results[url] = cache_data
                continue
        
        # DacÄƒ nu existÄƒ Ã®n cache sau a expirat, face cererea API
        try:
            logging.info(f"ğŸŒ Preiau date noi pentru {url[:50]}...")
            data = make_request_with_retry(url)
            
            # SalveazÄƒ rÄƒspunsul Ã®n cache
            set_cache_with_lock(url, (datetime.now(), data))
            
            results[url] = data
        except Exception as e:
            logging.error(f"âŒ Eroare la preluarea datelor Ã®n batch: {e}")
            
            # DacÄƒ avem date Ã®n cache, le returnÄƒm pe acelea chiar dacÄƒ au expirat
            cached_data = get_cache_with_lock(url)
            if USE_LAST_VALID_RESPONSE and cached_data:
                logging.warning(f"âš ï¸ Returnez ultimele date valide din cache pentru {url[:50]}")
                results[url] = cached_data[1]
            else:
                results[url] = {'error': str(e)}
    
    return jsonify(results)

@app.route('/api/cache/clear', methods=['POST'])
def clear_cache():
    """Endpoint pentru golirea cache-ului."""
    with cache_lock:
        cache.clear()
    
    # DupÄƒ golirea cache-ului, iniÈ›iem un prefetch pentru a reÃ®ncÄƒrca datele importante
    if PREFETCH_ENABLED:
        threading.Thread(target=prefetch_important_data).start()
    
    return jsonify({'success': True, 'message': 'Cache golit cu succes, repopulare date Ã®n curs'})

@app.route('/api/cache/status', methods=['GET'])
def cache_status():
    """Endpoint pentru verificarea stÄƒrii cache-ului."""
    with cache_lock:
        cache_entries = len(cache)
        cache_keys = list(cache.keys())[:10]  # Primele 10 chei pentru exemplificare
        
        # Calculam vechimea medie a datelor din cache
        now = datetime.now()
        cache_age_seconds = 0
        if cache_entries > 0:
            for key in cache:
                cache_time, _ = cache[key]
                cache_age_seconds += (now - cache_time).total_seconds()
            cache_age_seconds = cache_age_seconds / cache_entries
    
    with rate_limit_lock:
        rate_limit_stats = {}
        current_time = time.time()
        for domain, last_request_time in rate_limits.items():
            rate_limit_stats[domain] = {
                "last_request": int(current_time - last_request_time),
                "can_request": (current_time - last_request_time) >= RATE_LIMIT_INTERVAL
            }
    
    return jsonify({
        'success': True,
        'cache_entries': cache_entries,
        'sample_keys': cache_keys,
        'cache_age_avg_seconds': int(cache_age_seconds) if cache_entries > 0 else 0,
        'prefetch_enabled': PREFETCH_ENABLED,
        'prefetch_interval_minutes': cache_refresh_interval // 60,
        'rate_limits': rate_limit_stats,
        'last_update': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })

# RutÄƒ specificÄƒ pentru preÈ›urile CoinGecko - direct compatibilÄƒ cu aplicaÈ›ia existentÄƒ
@app.route('/api/prices', methods=['GET'])
def get_prices():
    """
    Endpoint specific pentru preÈ›urile CoinGecko, complet compatibil cu aplicaÈ›ia existentÄƒ.
    AcceptÄƒ È™i parametrul 'ids' pentru a filtra monedele.
    """
    coingecko_url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        "vs_currency": request.args.get('vs_currency', 'usd'),
        "order": request.args.get('order', 'market_cap_desc'),
        "per_page": request.args.get('per_page', '50'),  # MÄƒresc la 50 pentru a acoperi mai multe monede
        "page": request.args.get('page', '1'),
        "sparkline": request.args.get('sparkline', 'false'),
        "price_change_percentage": request.args.get('price_change_percentage', '24h')
    }
    
    # DacÄƒ sunt specificate ID-uri, le adÄƒugÄƒm la parametri
    if request.args.get('ids'):
        params['ids'] = request.args.get('ids')
    
    # ConstruieÈ™te URL-ul complet pentru cache key
    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
    cache_key = f"{coingecko_url}?{query_string}"
    
    # VerificÄƒ dacÄƒ rÄƒspunsul este Ã®n cache È™i dacÄƒ este Ã®ncÄƒ valid
    cached_data = get_cache_with_lock(cache_key)
    if cached_data:
        cache_time, cache_data = cached_data
        if datetime.now() < cache_time + timedelta(seconds=cache_duration):
            logging.info("âœ… Folosesc date din cache pentru preÈ›uri")
            return jsonify(cache_data)
    
    # AlternÄƒm Ã®ntre multiple surse de date pentru redundanÈ›Äƒ
    api_sources = [
        # SursÄƒ primarÄƒ: CoinGecko markets API - folosind proxy dedicat
        lambda: make_request_with_retry(coingecko_url, params),
        
        # SursÄƒ secundarÄƒ: CoinGecko simple price API
        lambda: convert_simple_price_format(
            make_request_with_retry(
                "https://api.coingecko.com/api/v3/simple/price",
                {
                    "ids": params.get('ids', 'bitcoin,ethereum'),
                    "vs_currencies": "usd",
                    "include_market_cap": "true",
                    "include_24hr_vol": "true",
                    "include_24hr_change": "true"
                }
            )
        ),
        
        # SursÄƒ terÈ›iarÄƒ: CryptoCompare API
        lambda: convert_cryptocompare_format(
            make_request_with_retry(
                "https://min-api.cryptocompare.com/data/pricemultifull",
                {
                    "fsyms": "BTC,ETH",
                    "tsyms": "USD"
                }
            )
        )
    ]
    
    # ÃncercÄƒm fiecare sursÄƒ pe rÃ¢nd
    last_error = None
    for source_index, source_fn in enumerate(api_sources):
        try:
            logging.info(f"ğŸŒ Ãncercarea #{source_index+1}: Preiau date de la sursa de preÈ›uri...")
            data = source_fn()
            
            # ValidÄƒm datele primite
            if not isinstance(data, list) or len(data) == 0:
                raise ValueError("Datele primite nu sunt Ã®n formatul aÈ™teptat")
            
            # SalveazÄƒ rÄƒspunsul Ã®n cache
            set_cache_with_lock(cache_key, (datetime.now(), data))
            
            return jsonify(data)
        except Exception as e:
            last_error = e
            logging.warning(f"âš ï¸ Sursa #{source_index+1} a eÈ™uat: {e}")
            continue  # ÃncercÄƒm urmÄƒtoarea sursÄƒ
    
    # Toate sursele au eÈ™uat, verificÄƒm dacÄƒ avem date Ã®n cache
    cached_data = get_cache_with_lock(cache_key)
    if USE_LAST_VALID_RESPONSE and cached_data:
        logging.warning(f"âš ï¸ Toate sursele au eÈ™uat. Returnez ultimele date valide din cache pentru preÈ›uri")
        return jsonify(cached_data[1])
    
    # Nu avem date Ã®n cache, returnÄƒm datele de backup
    logging.error(f"âŒ Toate sursele au eÈ™uat È™i nu existÄƒ date Ã®n cache. Returnez datele de backup.")
    return jsonify(FALLBACK_PRICES_DATA)

def convert_simple_price_format(simple_price_data):
    """ConverteÈ™te datele din formatul simple/price Ã®n formatul markets"""
    result = []
    
    if 'bitcoin' in simple_price_data:
        result.append({
            "id": "bitcoin",
            "symbol": "btc",
            "name": "Bitcoin",
            "image": "https://assets.coingecko.com/coins/images/1/small/bitcoin.png",
            "current_price": simple_price_data['bitcoin']['usd'],
            "market_cap": simple_price_data['bitcoin'].get('usd_market_cap', 0),
            "market_cap_rank": 1,
            "total_volume": simple_price_data['bitcoin'].get('usd_24h_vol', 0),
            "price_change_percentage_24h": simple_price_data['bitcoin'].get('usd_24h_change', 0),
            "sparkline_in_7d": {"price": []}
        })
    
    if 'ethereum' in simple_price_data:
        result.append({
            "id": "ethereum",
            "symbol": "eth",
            "name": "Ethereum",
            "image": "https://assets.coingecko.com/coins/images/279/small/ethereum.png",
            "current_price": simple_price_data['ethereum']['usd'],
            "market_cap": simple_price_data['ethereum'].get('usd_market_cap', 0),
            "market_cap_rank": 2,
            "total_volume": simple_price_data['ethereum'].get('usd_24h_vol', 0),
            "price_change_percentage_24h": simple_price_data['ethereum'].get('usd_24h_change', 0),
            "sparkline_in_7d": {"price": []}
        })
    
    return result

def convert_cryptocompare_format(cryptocompare_data):
    """ConverteÈ™te datele din formatul CryptoCompare Ã®n formatul CoinGecko markets"""
    result = []
    
    if 'RAW' in cryptocompare_data and 'BTC' in cryptocompare_data['RAW'] and 'USD' in cryptocompare_data['RAW']['BTC']:
        btc_data = cryptocompare_data['RAW']['BTC']['USD']
        result.append({
            "id": "bitcoin",
            "symbol": "btc",
            "name": "Bitcoin",
            "image": "https://assets.coingecko.com/coins/images/1/small/bitcoin.png",
            "current_price": btc_data.get('PRICE', 0),
            "market_cap": btc_data.get('MKTCAP', 0),
            "market_cap_rank": 1,
            "total_volume": btc_data.get('VOLUME24HOUR', 0),
            "price_change_percentage_24h": btc_data.get('CHANGEPCT24HOUR', 0),
            "sparkline_in_7d": {"price": []}
        })
    
    if 'RAW' in cryptocompare_data and 'ETH' in cryptocompare_data['RAW'] and 'USD' in cryptocompare_data['RAW']['ETH']:
        eth_data = cryptocompare_data['RAW']['ETH']['USD']
        result.append({
            "id": "ethereum",
            "symbol": "eth",
            "name": "Ethereum",
            "image": "https://assets.coingecko.com/coins/images/279/small/ethereum.png",
            "current_price": eth_data.get('PRICE', 0),
            "market_cap": eth_data.get('MKTCAP', 0),
            "market_cap_rank": 2,
            "total_volume": eth_data.get('VOLUME24HOUR', 0),
            "price_change_percentage_24h": eth_data.get('CHANGEPCT24HOUR', 0),
            "sparkline_in_7d": {"price": []}
        })
    
    return result

@app.errorhandler(404)
def page_not_found(e):
    return jsonify({"error": "Endpoint nu a fost gÄƒsit"}), 404

@app.errorhandler(500)
def internal_server_error(e):
    return jsonify({"error": "Eroare internÄƒ de server", "details": str(e)}), 500

if __name__ == '__main__':
    # InstaleazÄƒ pachetele necesare dacÄƒ lipsesc
    try:
        import schedule
    except ImportError:
        logging.info("Instalez pachetul 'schedule'...")
        import pip
        pip.main(['install', 'schedule'])
        import schedule
    
    # IniÈ›iem prefetch-ul È™i job-urile programate
    schedule_prefetch_jobs()
    
    print("ğŸš€ Server proxy pornit pe http://127.0.0.1:5000/")
    logging.info("Server proxy pornit")
    # Pentru producÈ›ie, foloseÈ™te un server WSGI precum Gunicorn
    app.run(debug=False, port=5000, host='127.0.0.1')